# Technical Documentation

## Architecture

Diff-Insight is structured into four main components:

1. **Diff Extraction** (`src/diff/`)
   - Fetches PR changes
   - Parses LaTeX environments
   - Maintains context
   - Handles binary files

2. **LLM Integration** (`src/llm/`)
   - Summarizes changes
   - Generates quizzes
   - Supports multiple backends
   - Handles rate limiting

3. **Output Generation** (`src/output/`)
   - Renders Markdown
   - Creates PDF diffs
   - Posts comments
   - Manages artifacts

4. **Configuration** (`src/config/`)
   - Loads settings
   - Manages tokens
   - Controls costs
   - Handles security

## Flow Diagram

\`\`\`mermaid
sequenceDiagram
    participant PR as Pull Request
    participant DI as Diff-Insight
    participant LLM as Language Model
    participant GH as GitHub API

    PR->>DI: Push Changes
    DI->>GH: Fetch Diff
    DI->>DI: Extract LaTeX Blocks
    DI->>LLM: Request Summary
    LLM->>DI: Return Analysis
    DI->>LLM: Generate Quiz
    LLM->>DI: Return Questions
    DI->>DI: Generate PDF Diff
    DI->>GH: Post Comment
\`\`\`

## Components

### Diff Extraction

The diff extractor uses these key classes:

\`\`\`typescript
interface LatexBlock {
  type: string;          // Environment type
  content: string[];     // Changed lines
  context: string[];     // Surrounding lines
  metadata: {
    title?: string;      // Section title
    label?: string;      // LaTeX label
    hasMath: boolean;    // Contains equations
    citations: string[]; // Referenced papers
  };
  path: string;          // File path
  lineStart: number;     // Starting line
  lineEnd: number;       // Ending line
}
\`\`\`

### LLM Integration

Supports multiple LLM backends:

\`\`\`typescript
interface LLMProvider {
  summarizeChanges(blocks: LatexBlock[]): Promise<Summary>;
  generateQuiz(summary: Summary): Promise<Quiz[]>;
}

interface Summary {
  new_claims: string[];
  changed_figures: string[];
  equations: string[];
  impact: string[];
}

interface Quiz {
  tf: {
    question: string;
    answer: boolean;
    explanation: string;
  };
  short: {
    question: string;
    answer: string;
    rubric: string;
  };
}
\`\`\`

### Output Generation

Comment structure:

\`\`\`typescript
interface Comment {
  header: {
    title: string;
    badges: Badge[];
  };
  sections: {
    claims: Section;
    figures: Section;
    math: Section;
    impact: Section;
  };
  quiz: Quiz[];
  sources?: Record<string, string>;
  pdfDiff?: string;
}

interface Section {
  title: string;
  content: string[];
  expanded: boolean;
}
\`\`\`

## Security

### Token Management

- GitHub tokens stored as secrets
- API keys validated at startup
- Rate limiting per repository
- Token usage tracking

### Data Privacy

- No external data storage
- Local LLM option
- Sanitized logging
- Secure artifact handling

## Performance

### Optimization

1. **Caching**
   - LLM responses cached
   - PDF diffs stored
   - Comment updates batched

2. **Rate Limiting**
   - Token bucket algorithm
   - Configurable limits
   - Automatic retries
   - Error backoff

3. **Resource Usage**
   - Chunked processing
   - Memory monitoring
   - Garbage collection
   - Stream handling

### Metrics

| Metric | Target | Current |
|--------|--------|---------|
| PR Processing | < 30s | 25s avg |
| Memory Usage | < 512MB | 300MB avg |
| Token Cost | < 5K/PR | 2K avg |
| Success Rate | > 99% | 99.5% |

## Testing

### Coverage Requirements

- Lines: 80%
- Functions: 80%
- Branches: 80%
- Statements: 80%

### Test Types

1. **Unit Tests**
   - Component isolation
   - Mock dependencies
   - Edge cases
   - Error handling

2. **Integration Tests**
   - Full workflow
   - API interaction
   - Real data
   - Error recovery

3. **Performance Tests**
   - Load testing
   - Memory leaks
   - Concurrency
   - Rate limits

## Deployment

### GitHub Action

\`\`\`yaml
name: Diff-Insight
runs:
  using: 'node20'
  main: 'dist/index.js'
\`\`\`

### Requirements

- Node.js â‰¥ 20
- GitHub Actions
- LaTeX installation
- API access

### Configuration

\`\`\`typescript
interface Config {
  github: {
    token: string;
    maxFiles: number;
  };
  llm: {
    provider: 'openai' | 'local';
    model: string;
    endpoint?: string;
  };
  limits: {
    tokens: number;
    requests: number;
    size: number;
  };
}
\`\`\`

## Error Handling

### Strategy

1. **Retryable Errors**
   - Network timeouts
   - Rate limits
   - Temporary failures

2. **Fatal Errors**
   - Invalid tokens
   - Missing files
   - Format errors

3. **Recovery**
   - Automatic retry
   - Graceful degradation
   - Clear messaging

### Error Types

\`\`\`typescript
class DiffError extends Error {
  code: string;
  retryable: boolean;
  context: any;
}

class LLMError extends Error {
  code: string;
  tokens: number;
  request: any;
}
\`\`\`

## Future Improvements

1. **Features**
   - Multiple LLM support
   - Custom quiz templates
   - Interactive comments
   - Batch processing

2. **Performance**
   - Parallel processing
   - Better caching
   - Streaming responses
   - Resource optimization

3. **Integration**
   - CI/CD pipelines
   - IDE plugins
   - Custom workflows
   - Analytics